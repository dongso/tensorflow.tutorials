{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks with regularization\n",
    "\n",
    "* Make a networks like LeNet5 structure with MNIST data\n",
    "* input pipeline: `tf.data`\n",
    "* `Eager execution`\n",
    "* `Functional API`\n",
    "* Apply various regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:17.758924Z",
     "start_time": "2019-02-27T13:56:12.125620Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # this statement and below codes are the same effects.\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:19.845355Z",
     "start_time": "2019-02-27T13:56:17.772006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_data = train_data.reshape([-1, 28, 28, 1])\n",
    "train_data = train_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_data = test_data.reshape([-1, 28, 28, 1])\n",
    "test_data = test_data.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:20.473198Z",
     "start_time": "2019-02-27T13:56:19.862713Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 219\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index][...,0])\n",
    "plt.colorbar()\n",
    "#plt.gca().grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### input pipeline `tf.data.Dataset` and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:20.526599Z",
     "start_time": "2019-02-27T13:56:20.486097Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(219)\n",
    "batch_size = 32\n",
    "max_epochs = 1\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.batch(batch_size = batch_size)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "* Use `tf.keras.layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `tf.keras.Sequential()` API (01.mnist.LeNet5.ipynb)\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Conv2D(filters=32, kernel_size=[5, 5], padding='same', activation='relu'),\n",
    "  layers.MaxPool2D(),\n",
    "  layers.Conv2D(filters=64, kernel_size=[5, 5], padding='same', activation='relu'),\n",
    "  layers.MaxPool2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(1024, activation='relu'),\n",
    "  layers.Dense(10)])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:20.577714Z",
     "start_time": "2019-02-27T13:56:20.537295Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.l2_decay = 0.001\n",
    "    self.conv1 = layers.Conv2D(filters=32, kernel_size=[5, 5], padding='same',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(self.l2_decay))\n",
    "    self.conv1_bn = layers.BatchNormalization()\n",
    "    self.pool1 = layers.MaxPool2D()\n",
    "    self.conv2 = layers.Conv2D(filters=64, kernel_size=[5, 5], padding='same',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(self.l2_decay))\n",
    "    self.conv2_bn = layers.BatchNormalization()\n",
    "    self.pool2 = layers.MaxPool2D()\n",
    "    self.flatten = layers.Flatten()\n",
    "    self.dense1 = layers.Dense(units=1024,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(self.l2_decay))\n",
    "    self.dense1_bn = layers.BatchNormalization()\n",
    "    self.drop1 = layers.Dropout(rate=0.6)\n",
    "    self.dense2 = layers.Dense(units=10, kernel_regularizer=tf.keras.regularizers.l2(self.l2_decay))\n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    self.conv1_ = self.conv1(inputs)\n",
    "    self.conv1_bn_ = self.conv1_bn(self.conv1_, training=training)\n",
    "    self.conv1_ = tf.nn.relu(self.conv1_bn_)\n",
    "    self.pool1_ = self.pool1(self.conv1_)\n",
    "    \n",
    "    self.conv2_ = self.conv2(self.pool1_)\n",
    "    self.conv2_bn_ = self.conv2_bn(self.conv2_, training=training)\n",
    "    self.conv2_ = tf.nn.relu(self.conv2_bn_)\n",
    "    self.pool2_ = self.pool2(self.conv2_)\n",
    "    \n",
    "    self.flatten_ = self.flatten(self.pool2_)\n",
    "    self.dense1_ = self.dense1(self.flatten_)\n",
    "    self.dense1_bn_ = self.dense1_bn(self.dense1_, training=training)\n",
    "    self.dense1_ = tf.nn.relu(self.dense1_bn_)\n",
    "    self.drop1_ = self.drop1(self.dense1_, training=training)\n",
    "    self.logits_ = self.dense2(self.drop1_)\n",
    "    \n",
    "    return self.logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:20.610162Z",
     "start_time": "2019-02-27T13:56:20.583570Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:21.118035Z",
     "start_time": "2019-02-27T13:56:20.614133Z"
    }
   },
   "outputs": [],
   "source": [
    "# without training, just inference a model in eager execution:\n",
    "for images, labels in train_dataset.take(1):\n",
    "  print(\"Logits: \", model(images[0:3], training=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:21.138367Z",
     "start_time": "2019-02-27T13:56:21.123638Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "### Define a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:21.160098Z",
     "start_time": "2019-02-27T13:56:21.150563Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "cross_entropy_history = []\n",
    "l2_loss_history = []\n",
    "total_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:06:24.169140Z",
     "start_time": "2019-02-27T13:56:53.826033Z"
    }
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "for epoch in range(max_epochs):\n",
    "  for images, labels in train_dataset:\n",
    "    start_time = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(images, training=True)\n",
    "      cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "      l2_loss = tf.reduce_sum(model.losses)\n",
    "      total_loss = cross_entropy + l2_loss\n",
    "\n",
    "    cross_entropy_history.append(cross_entropy.numpy())\n",
    "    l2_loss_history.append(l2_loss.numpy())\n",
    "    total_loss_history.append(total_loss.numpy())\n",
    "    \n",
    "    grads = tape.gradient(total_loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                              global_step=global_step)\n",
    "\n",
    "    if global_step.numpy() % 10 == 0:\n",
    "      clear_output(wait=True)\n",
    "      duration = time.time() - start_time\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      epochs = batch_size * global_step.numpy() / float(len(train_data))\n",
    "      print(\"epochs: {:.2f}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs, global_step.numpy(), total_loss, examples_per_sec, duration))\n",
    "\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:07:01.262211Z",
     "start_time": "2019-02-27T14:07:00.704011Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n",
    "#plt.set_title(\"cross_entropy\")\n",
    "plt.plot(cross_entropy_history, label='cross_entopy')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Loss value [cross entropy]')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n",
    "#plt.set_title(\"l2_loss\")\n",
    "plt.plot(l2_loss_history, label='l2_loss')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Loss value [l2 loss]')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n",
    "#plt.set_title(\"total_loss\")\n",
    "plt.plot(total_loss_history, label='total_loss')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Total loss value [cross entropy + l2 loss]')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model\n",
    "\n",
    "### Test trained model\n",
    "\n",
    "* test accuracy: 0.9798 for 1 epochs (without regularization)\n",
    "* test accuracy: 0.9869 for 1 epochs (with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:07:33.148850Z",
     "start_time": "2019-02-27T14:07:04.865351Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "  logits = model(images, training=False)\n",
    "  accuracy(labels=labels, predictions=tf.cast(tf.argmax(logits, 1), tf.int32))\n",
    "  \n",
    "print(\"test accuracy: {}\".format(accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:22.869472Z",
     "start_time": "2019-02-27T13:56:12.732Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:56:22.872086Z",
     "start_time": "2019-02-27T13:56:12.737Z"
    }
   },
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "\n",
    "batch_xs = test_data[batch_index]\n",
    "batch_ys = test_labels[batch_index]\n",
    "y_pred_ = model(batch_xs, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred_)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  if np.argmax(py) == batch_ys[i]:\n",
    "    p.set_title(\"y_pred: {}\".format(np.argmax(py)), color='blue')\n",
    "  else:\n",
    "    p.set_title(\"y_pred: {}\".format(np.argmax(py)), color='red')\n",
    "  p.imshow(px.reshape(28, 28))\n",
    "  p.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:29.629364Z",
     "start_time": "2019-02-27T14:08:29.625540Z"
    }
   },
   "source": [
    "### Print all feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:39.442900Z",
     "start_time": "2019-02-27T14:08:39.431946Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_all_feature_maps(layer, layer_name):\n",
    "  \"\"\"Print all feature maps\n",
    "    This code is borrowed from \"Deep Learning with Python\" (by F. Chollet)\n",
    "  \n",
    "  Args:\n",
    "    layer (4-rank Tensor): feature maps\n",
    "    layer_name (string): name of feature maps\n",
    "    \n",
    "  Returns:\n",
    "    print all feature maps\n",
    "  \"\"\"\n",
    "  num_features = layer.shape[-1]\n",
    "  size = int(layer.shape[1])\n",
    "  images_per_row = 16\n",
    "  for feature_map in range(num_features):\n",
    "    num_cols = num_features // images_per_row\n",
    "    display_grid = np.zeros((size * num_cols, images_per_row * size))\n",
    "\n",
    "    for col in range(num_cols):\n",
    "      for row in range(images_per_row):\n",
    "        channel_image = layer[0,:,:,col * images_per_row + row]\n",
    "\n",
    "        #channel_image -= channel_image.mean()\n",
    "        channel_image -= tf.reduce_mean(channel_image)\n",
    "        #channel_image /= channel_image.std()\n",
    "        channel_image /= tf.keras.backend.std(channel_image)\n",
    "        channel_image *= 64\n",
    "        channel_image += 128\n",
    "        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "\n",
    "        display_grid[col * size : (col + 1) * size,\n",
    "                     row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "  scale = 1. / size\n",
    "  plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                      scale * display_grid.shape[0]))\n",
    "  plt.title(layer_name)\n",
    "  plt.grid(False)\n",
    "  plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:42.783455Z",
     "start_time": "2019-02-27T14:08:42.345091Z"
    }
   },
   "outputs": [],
   "source": [
    "# without training, just inference a model in eager execution:\n",
    "for images, labels in train_dataset.take(1):\n",
    "  print(\"Logits: \", model(images[10:11], training=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:44.389791Z",
     "start_time": "2019-02-27T14:08:43.736407Z"
    }
   },
   "outputs": [],
   "source": [
    "print_all_feature_maps(model.conv1_, 'conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:48.225381Z",
     "start_time": "2019-02-27T14:08:46.383766Z"
    }
   },
   "outputs": [],
   "source": [
    "print_all_feature_maps(model.conv2_, 'conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:49.966430Z",
     "start_time": "2019-02-27T14:08:49.959870Z"
    }
   },
   "outputs": [],
   "source": [
    "model.drop1_[0,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:52.855915Z",
     "start_time": "2019-02-27T14:08:52.800277Z"
    }
   },
   "outputs": [],
   "source": [
    "# without training, just inference a model in eager execution:\n",
    "for images, labels in train_dataset.take(1):\n",
    "  print(\"Logits: \", model(images[10:11], training=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T14:08:55.513101Z",
     "start_time": "2019-02-27T14:08:55.506362Z"
    }
   },
   "outputs": [],
   "source": [
    "model.drop1_[0,:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
