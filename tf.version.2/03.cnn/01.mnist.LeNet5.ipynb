{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks\n",
    "\n",
    "* Make a networks like LeNet5 structure with MNIST data\n",
    "* input pipeline: `tf.data`\n",
    "* `Eager execution`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:36.901256Z",
     "start_time": "2019-02-27T12:49:34.180892Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:37.717712Z",
     "start_time": "2019-02-27T12:49:36.905690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_data = train_data.reshape([-1, 28, 28, 1])\n",
    "train_data = train_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_data = test_data.reshape([-1, 28, 28, 1])\n",
    "test_data = test_data.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:37.981891Z",
     "start_time": "2019-02-27T12:49:37.720200Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 219\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index][...,0])\n",
    "plt.colorbar()\n",
    "#plt.gca().grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### input pipeline `tf.data.Dataset` and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:38.010648Z",
     "start_time": "2019-02-27T12:49:37.987501Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(219)\n",
    "batch_size = 32\n",
    "max_epochs = 1\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.batch(batch_size = batch_size)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "* Use `tf.keras.layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:38.034877Z",
     "start_time": "2019-02-27T12:49:38.017253Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Conv2D(filters=32, kernel_size=[5, 5],\n",
    "                padding='same', activation='relu'),\n",
    "  layers.MaxPool2D(),\n",
    "  layers.Conv2D(filters=64, kernel_size=[5, 5],\n",
    "                padding='same', activation='relu'),\n",
    "  layers.MaxPool2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(1024, activation='relu'),\n",
    "  layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:38.169114Z",
     "start_time": "2019-02-27T12:49:38.038776Z"
    }
   },
   "outputs": [],
   "source": [
    "# without training, just inference a model in eager execution:\n",
    "for images, labels in train_dataset.take(1):\n",
    "  print(\"Logits: \", model(images[0:3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:38.178695Z",
     "start_time": "2019-02-27T12:49:38.171253Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "### Define a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:49:38.190617Z",
     "start_time": "2019-02-27T12:49:38.184551Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:52:50.716780Z",
     "start_time": "2019-02-27T12:49:38.193308Z"
    }
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "for epoch in range(max_epochs):\n",
    "  for (step, (images, labels)) in enumerate(train_dataset):\n",
    "    start_time = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(images, training=True)\n",
    "      loss_value = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "\n",
    "    loss_history.append(loss_value.numpy())\n",
    "    grads = tape.gradient(loss_value, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                              global_step=global_step)\n",
    "\n",
    "    if global_step.numpy() % 100 == 0:\n",
    "      clear_output(wait=True)\n",
    "      duration = time.time() - start_time\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      epochs = batch_size * global_step.numpy() / float(len(train_data))\n",
    "      print(\"epochs: {:.2f}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs, global_step.numpy(), loss_value, examples_per_sec, duration))\n",
    "\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:52:50.921893Z",
     "start_time": "2019-02-27T12:52:50.718949Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history, label='loss')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Loss value [cross entropy]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model\n",
    "\n",
    "### Test trained model\n",
    "\n",
    "* test accuracy: 0.9798 for 1 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:53:05.445519Z",
     "start_time": "2019-02-27T12:52:50.925487Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "  logits = model(images, training=False)\n",
    "  accuracy(labels=labels, predictions=tf.cast(tf.argmax(logits, 1), tf.int32))\n",
    "  \n",
    "print(\"test accuracy: {}\".format(accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:53:05.451783Z",
     "start_time": "2019-02-27T12:53:05.447924Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T12:53:06.451038Z",
     "start_time": "2019-02-27T12:53:05.454873Z"
    }
   },
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "\n",
    "batch_xs = test_data[batch_index]\n",
    "batch_ys = test_labels[batch_index]\n",
    "y_pred_ = model(batch_xs, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred_)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  if np.argmax(py) == batch_ys[i]:\n",
    "    p.set_title(\"y_pred: {}\".format(np.argmax(py)), color='blue')\n",
    "  else:\n",
    "    p.set_title(\"y_pred: {}\".format(np.argmax(py)), color='red')\n",
    "  p.imshow(px.reshape(28, 28))\n",
    "  p.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
